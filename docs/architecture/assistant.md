# Assistant (assistant/)

LLM-слой. Google Gemini с tool calling. Переводит естественный язык в JSON-запросы Query Engine.

## Как работает

```
Пользователь: "Какой средний дневной диапазон NQ за 2024?"
    ↓
Gemini получает system prompt + историю + сообщение
    ↓
Gemini вызывает understand_question → узнаёт возможности движка
    ↓
Gemini вызывает get_query_reference → узнаёт синтаксис запросов
    ↓
Gemini вызывает execute_query({...}) → получает результат
    ↓
Gemini формулирует ответ на человеческом языке
```

Максимум 5 раундов tool calling за один запрос. Обычно 1-3.

## Модули

### chat.py
Класс `Assistant`. Stateless — клиент передаёт полную историю каждый запрос. Считает токены, собирает tool_calls (имя, input, output, error, duration_ms), собирает data blocks для UI.

### prompt.py
Строит system prompt из конфига инструмента. Включает: роль, контекст (символ, биржа, диапазон данных, сессии), инструкции по воркфлоу. Промпт маленький — детали LLM получает через тулы по запросу.

### tools/
Три инструмента:

- **understand_question** — возвращает описание возможностей движка (таймфреймы, сессии, функции, ограничения). LLM вызывает первым чтобы понять что можно считать.
- **get_query_reference** — возвращает полную спецификацию формата запросов (все поля, функции, порядок выполнения, примеры). LLM использует чтобы составить правильный JSON.
- **execute_query** — принимает JSON-запрос, валидирует, выполняет через interpreter, возвращает результат + метаданные.

## Дизайн

**Промпт маленький.** Всё что LLM нужно знать "по запросу" — в тулах. Промпт содержит только то что нужно для каждого ответа. Это снижает latency и стоимость.

**Tool calls логируются.** Каждый вызов записывается с duration_ms и error — для анализа качества ответов и дебага.
