# Result Format: Model vs UI

## Задача

Определить что получает модель (result) и что видит пользователь (table/source_rows) для ВСЕХ возможных комбинаций операций.

---

## Текущая реализация (AS-IS)

### Что происходит сейчас

**1. Interpreter (`barb/interpreter.py`) возвращает:**
```python
{
    "result": table или scalar,      # результат запроса
    "table": [...] или None,         # те же данные если таблица
    "source_rows": [...],            # строки до агрегации
    "metadata": {...},
    "query": {...}
}
```

**2. Tool (`assistant/tools/__init__.py`) форматирует для модели:**
```python
# Если таблица:
return f"Result: {len(rows)} rows\n{json.dumps(rows, indent=2)}"

# Если скаляр:
return f"Result: {result}"
```

**3. Chat (`assistant/chat.py`) парсит data_blocks:**
```python
if "rows" in result_str:
    table_data = json.loads(lines[1])  # парсим JSON обратно
    data_blocks.append({
        "tool": tool_name,
        "input": {"query": query},
        "result": table_data,    # ПОЛНЫЕ ДАННЫЕ
        "rows": len(table_data)
    })
```

**4. API (`api/main.py`) сохраняет в Supabase:**
```python
db.table("messages").insert({
    "content": result["answer"],
    "data": result["data"],  # data_blocks с полными данными
})
```

### Проблема

```
┌─────────────────────────────────────────────────────────────────┐
│                     СЕЙЧАС                                      │
├─────────────────────────────────────────────────────────────────┤
│ Модель получает:  "Result: 13 rows\n[{...полный JSON...}]"     │
│ UI получает:      те же данные из messages.data                 │
│ Supabase хранит:  полные данные в messages.data                 │
└─────────────────────────────────────────────────────────────────┘

Модель и UI получают ОДИНАКОВЫЕ данные.
Нет разделения на result (для модели) и table (для UI).
```

### Что это значит

1. **Токены тратятся зря** — модель получает 500 токенов JSON когда ей нужно только "13 rows, min=-5.06"
2. **Модель может запутаться** — слишком много данных
3. **Нет архитектурного разделения** — result = table = одно и то же

---

## Целевая архитектура (TO-BE)

### Принцип разделения

```
┌─────────────────────────────────────────────────────────────────┐
│                     ДОЛЖНО БЫТЬ                                 │
├─────────────────────────────────────────────────────────────────┤
│ Модель получает:  "Result: 13 rows, change_pct: min=-5.06..."  │
│                   (компактные метаданные)                       │
├─────────────────────────────────────────────────────────────────┤
│ UI получает:      полная таблица 13 строк                       │
│                   (через SSE event data_block)                  │
├─────────────────────────────────────────────────────────────────┤
│ Supabase хранит:  полные данные для истории                     │
│                   (messages.data)                               │
└─────────────────────────────────────────────────────────────────┘
```

### Поток данных

```
Interpreter                    Tool                      Chat                     API
     │                          │                         │                        │
     │  {                       │                         │                        │
     │    result: summary,      │                         │                        │
     │    table: [...rows],     │   "Result: summary"     │   SSE: data_block      │
     │    source_rows: [...]    │ ──────────────────────► │ ─────────────────────► │  UI
     │  }                       │   (для модели)          │   {table: rows}        │
     │                          │                         │                        │
     │                          │                         │   SSE: done            │
     │                          │                         │ ─────────────────────► │  Supabase
     │                          │                         │   {data: rows}         │  messages.data
```

### Что меняется

| Компонент | Сейчас | Должно быть |
|-----------|--------|-------------|
| `interpreter.py` | result = table | result = summary, table = данные |
| `tools/__init__.py` | JSON dump всех строк | Форматирование summary |
| `chat.py` | Парсит JSON обратно | Передаёт table напрямую |
| `api/main.py` | Сохраняет data_blocks | Без изменений |

### Структура ответа интерпретатора

```python
# Новый формат ответа execute()
{
    # Для модели — компактный summary
    "summary": {
        "type": "table",           # scalar | dict | table | grouped
        "rows": 13,
        "columns": ["timestamp", "open", "close", "change_pct"],
        "stats": {
            "change_pct": {"min": -5.06, "max": -2.51, "mean": -3.27}
        },
        "first": {"timestamp": "2025-04-08", "change_pct": -5.06},
        "last": {"timestamp": "2024-09-03", "change_pct": -2.51},
    },

    # Для UI — полные данные
    "table": [
        {"timestamp": "2025-04-08", "open": 18150.5, ..., "change_pct": -5.06},
        {"timestamp": "2025-11-20", ...},
        ...
    ],

    # Для drill-down (опционально)
    "source_rows": [...],  # строки до агрегации

    # Метаданные
    "metadata": {
        "rows_scanned": 500,
        "session": "RTH",
        "timeframe": "daily"
    },

    "query": {...}
}
```

### Форматирование для модели

```python
# tools/__init__.py
def format_for_model(summary: dict) -> str:
    """Форматирует summary в строку для модели."""

    if summary["type"] == "scalar":
        return f"Result: {summary['value']}"

    if summary["type"] == "dict":
        vals = ", ".join(f"{k}={v}" for k, v in summary["values"].items())
        return f"Result: {vals}"

    if summary["type"] == "table":
        lines = [f"Result: {summary['rows']} rows"]
        if summary.get("stats"):
            for col, st in summary["stats"].items():
                lines.append(f"  {col}: min={st['min']}, max={st['max']}, mean={st['mean']:.2f}")
        if summary.get("first"):
            lines.append(f"  first: {summary['first']}")
        if summary.get("last"):
            lines.append(f"  last: {summary['last']}")
        return "\n".join(lines)

    if summary["type"] == "grouped":
        lines = [f"Result: {summary['rows']} groups by {summary['by']}"]
        if summary.get("min"):
            lines.append(f"  min: {summary['min']}")
        if summary.get("max"):
            lines.append(f"  max: {summary['max']}")
        return "\n".join(lines)
```

---

## Шаг 1: Что делает интерпретатор?

Pipeline выполняется строго по порядку:

```
1. SESSION  → фильтр минуток по времени дня
2. PERIOD   → фильтр по диапазону дат
3. FROM     → ресемпл в таймфрейм (минутки → daily/weekly/etc)
4. MAP      → добавить вычисляемые колонки
5. WHERE    → фильтр строк по условию
6. GROUP_BY → группировка
7. SELECT   → агрегация
8. SORT     → сортировка
9. LIMIT    → ограничение количества строк
```

**Ключевое наблюдение:**
- Шаги 1-5 работают со строками (фильтруют, трансформируют)
- Шаги 6-7 агрегируют (уменьшают количество строк или превращают в скаляр)
- Шаги 8-9 форматируют вывод

---

## Шаг 2: Какие данные существуют?

После шага 5 (WHERE) у нас есть **отфильтрованные строки**:
```
Пример: 80 дневных баров с колонками [timestamp, open, high, low, close, volume, gap, range]
```

Это "source data" — данные которые участвуют в расчёте.

После шагов 6-7 (GROUP_BY + SELECT) данные **агрегируются**:
- Либо в скаляр: `count() → 80`
- Либо в сгруппированную таблицу: `group_by dow → 5 строк`
- Либо остаются как есть (если нет select/group_by)

---

## Шаг 3: Все комбинации select/group_by

В коде интерпретатора (строки 106-119):

```python
if group_by:
    select = select_raw or "count()"  # group_by требует select
    result_df = _group_aggregate(df, group_by, select)
elif select_raw:
    result_df = _aggregate(df, select)  # скаляр или dict
else:
    result_df = df  # просто строки
```

**Возможные комбинации:**

| select | group_by | Результат |
|--------|----------|-----------|
| - | - | DataFrame (строки как есть) |
| scalar | - | скаляр (одно число) |
| [multi] | - | dict (несколько чисел) |
| любой | ✓ | DataFrame (сгруппированный) |

---

## Шаг 4: Роль sort и limit

**sort** — упорядочивает строки. Имеет смысл только для DataFrame (не для скаляра).

**limit** — ограничивает количество строк. Имеет смысл только для DataFrame.

**Важно:** limit применяется ПОСЛЕ group_by/select, то есть ограничивает финальный результат.

---

## Шаг 5: Полная матрица всех случаев

| # | select | group_by | Тип результата | Пример |
|---|--------|----------|----------------|--------|
| A | - | - | таблица строк | "покажи все дни где gap > 50" |
| B | scalar | - | одно число | "сколько дней где gap > 50" |
| C | [list] | - | несколько чисел | "покажи count, mean, max" |
| D | scalar | ✓ | сгруппированная таблица | "средний gap по дням недели" |
| E | [list] | ✓ | сгруппированная таблица | "count и mean по дням недели" |

**С учётом sort/limit:**

| # | Базовый тип | + sort | + limit | Итог |
|---|-------------|--------|---------|------|
| A | таблица | упорядочена | обрезана | таблица N строк |
| B | скаляр | игнор | игнор | скаляр |
| C | dict | игнор | игнор | dict |
| D | группы | упорядочены | обрезаны | таблица групп |
| E | группы | упорядочены | обрезаны | таблица групп |

---

## Шаг 6: Что является "доказательством" для пользователя?

**Вопрос:** Что пользователь хочет видеть как подтверждение результата?

### Случай A: таблица строк (нет агрегации)

```
Query: where: "gap > 50"
Result: 80 строк
```

Доказательство = сам результат. Пользователь видит строки которые совпали с условием.

```
result = 80 строк
source_rows = те же 80 строк (или не нужен отдельно?)
```

**Вывод:** result и source_rows — одно и то же. Нет агрегации = нет разделения.

### Случай B: скаляр

```
Query: where: "gap > 50", select: "count()"
Result: 80
```

Доказательство = строки которые посчитали.

```
result = 80 (число)
source_rows = 80 строк которые посчитали
```

**Вывод:** source_rows нужен как доказательство откуда взялось число.

### Случай C: dict (несколько скаляров)

```
Query: where: "gap > 50", select: ["count()", "mean(gap)", "max(gap)"]
Result: {count: 80, mean: 67.3, max: 156.2}
```

Доказательство = те же 80 строк.

```
result = {count: 80, mean: 67.3, max: 156.2}
source_rows = 80 строк
```

### Случай D: группировка со скаляром

```
Query: map: {dow: "dayname()"}, where: "gap > 50", group_by: "dow", select: "mean(gap)"
Result: 5 строк (Mon-Fri с средними)
```

Тут ДВА уровня данных:
1. Сгруппированный результат (5 строк)
2. Исходные данные до группировки (80 строк)

Что является доказательством?
- Опция 1: только 5 строк (результат группировки)
- Опция 2: 5 строк + 80 исходных (drill-down)

**Вопрос:** Нужен ли drill-down в исходные данные?

### Случай E: группировка с несколькими агрегатами

Аналогично D, просто больше колонок в результате.

---

## Шаг 7: Нужен ли source_rows для группировки?

**Аргументы ЗА:**
- Пользователь может захотеть посмотреть какие конкретно дни вошли в "пятницу"
- Это настоящее "доказательство" расчёта

**Аргументы ПРОТИВ:**
- Усложняет UI
- 80 строк могут быть слишком много
- Группировка сама по себе — это и есть результат

**Компромисс:**
- table = сгруппированный результат (5 строк) — показываем сразу
- source_rows = исходные данные (80 строк) — доступны по запросу

---

## Шаг 8: Что отдаём модели?

Модель должна написать осмысленный ответ, не видя сырых данных.

| Случай | Модель получает | Достаточно для ответа? |
|--------|-----------------|----------------------|
| A (таблица) | "80 rows, gap: min=51, max=156, mean=67" | Да |
| B (скаляр) | "80" | Да |
| C (dict) | "count=80, mean=67.3, max=156.2" | Да |
| D (группы) | "5 groups by dow, min=Fri(45), max=Mon(78)" | Да |
| E (группы) | "5 groups by dow, ..." | Да |

**Принцип:** модели достаточно метаданных (count, min, max, mean) чтобы описать результат.

---

## Шаг 9: Финальная схема

### Для случая A (таблица строк):

```
result (модель):   {type: "table", rows: 80, stats: {gap: {min: 51, max: 156}}}
table (UI):        80 строк полностью
source_rows:       не нужен (table = source)
```

### Для случая B (скаляр):

```
result (модель):   {type: "scalar", value: 80, rows_scanned: 500}
table (UI):        null (нет таблицы)
source_rows (UI):  80 строк которые посчитали
```

### Для случая C (dict):

```
result (модель):   {type: "dict", values: {count: 80, mean: 67.3, max: 156.2}}
table (UI):        null
source_rows (UI):  80 строк
```

### Для случая D/E (группировка):

```
result (модель):   {type: "grouped", rows: 5, by: "dow", min: {dow: "Fri", mean_gap: 45}, max: {dow: "Mon", mean_gap: 78}}
table (UI):        5 строк сгруппированных
source_rows (UI):  80 строк до группировки (опционально, drill-down)
```

---

## Шаг 10: Открытые вопросы

1. **stats для таблицы** — по каким колонкам считать min/max/mean?
   - Все числовые?
   - Только из map?
   - Только упомянутые в sort?

2. **source_rows для группировки** — обязателен или опционален?

3. **Большие таблицы** — если result это 500 строк, что делаем?
   - Всегда лимитировать?
   - Требовать limit от пользователя?

4. **Формат для модели** — JSON или текстовая строка?

---

## TODO

### Изменения в коде

1. **`barb/interpreter.py`**
   - [ ] Добавить генерацию `summary` в `_build_response()`
   - [ ] summary содержит: type, rows, stats (min/max/mean), first, last
   - [ ] Для grouped: добавить by, min_row, max_row

2. **`assistant/tools/__init__.py`**
   - [ ] Использовать `summary` для форматирования ответа модели
   - [ ] Не сериализовать полную таблицу в строку
   - [ ] Передавать `table` отдельно для data_block

3. **`assistant/chat.py`**
   - [ ] Убрать парсинг JSON из строки tool result
   - [ ] Получать table напрямую от tool
   - [ ] data_block формировать из table, не из parsed string

4. **Тесты**
   - [ ] Тест на каждый тип summary (scalar, dict, table, grouped)
   - [ ] Тест что модель НЕ получает сырые данные
   - [ ] Тест что UI получает полные данные

### Решения по открытым вопросам

#### 1. Stats для каких колонок?

**Решение: колонки из `map` + колонка из `sort`**

Логика:
- Если пользователь создал колонку в map — она ему важна
- Если сортирует по колонке — она ключевая для ответа

```python
stats_columns = set(query.get("map", {}).keys())
if query.get("sort"):
    sort_col = query["sort"].split()[0]
    stats_columns.add(sort_col)
```

Пример: `map: {change_pct: "..."}, sort: "change_pct desc"`
→ stats: `{change_pct: {min: -5.06, max: -2.51, mean: -3.27}}`

#### 2. First/last — какие поля?

**Решение: timestamp + колонки из `map`**

Логика:
- timestamp нужен всегда (когда это было)
- колонки из map — то что важно пользователю
- OHLCV не включаем (можно посмотреть в table)

```python
first_last_columns = ["timestamp"] + list(query.get("map", {}).keys())
```

Пример: `map: {change_pct: "..."}`
→ first: `{timestamp: "2025-04-08", change_pct: -5.06}`

#### 3. Source_rows — когда нужен?

**Решение:**

| Тип результата | source_rows | Причина |
|----------------|-------------|---------|
| Scalar/dict | ДА | Доказательство откуда число |
| Grouped | НЕТ (пока) | table сам по себе результат, drill-down усложняет |
| Table | НЕТ | table = source, дублирование |

---

## Итоговая архитектура

### Принцип

```
┌─────────────────────────────────────────────────────────────────┐
│  Любой вопрос = Данные (доказательство) + Комментарий (модель) │
├─────────────────────────────────────────────────────────────────┤
│  UI получает:     ПОЛНЫЕ данные (table / source_rows)          │
│  Модель получает: МИНИМУМ для комментария (summary)            │
└─────────────────────────────────────────────────────────────────┘
```

### Примеры

| Вопрос | summary (модель) | table/source (UI) |
|--------|------------------|-------------------|
| "сколько inside days?" | `{type: scalar, value: 65}` | 65 строк |
| "средний gap по dow" | `{type: grouped, rows: 5, min: Fri, max: Mon}` | 5 строк |
| "дни где упало >2.5%" | `{type: table, rows: 13, stats: {...}}` | 13 строк |
| "топ-5 падений" | `{type: table, rows: 5, first: {...}}` | 5 строк |

### Что пишет модель

На основе summary модель генерирует короткий комментарий:

- Scalar: "65 inside days за период"
- Grouped: "Пятница самая тихая (45), понедельник самый волатильный (78)"
- Table: "13 дней с падением >2.5%, худший — 8 апреля 2025 (−5.06%)"

Модель НЕ видит сырые данные. Только метаданные достаточные для осмысленного ответа.
